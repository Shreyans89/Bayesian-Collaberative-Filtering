{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8050e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "import sklearn\n",
    "from tqdm import tqdm\n",
    "import sklearn.metrics \n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efb0468f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]\n",
      "Pandas version: 1.3.5\n",
      "PySpark version: 3.1.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import pyspark\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import FloatType, IntegerType, LongType\n",
    "from recommenders.utils.spark_utils import start_or_get_spark\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))\n",
    "print(\"PySpark version: {}\".format(pyspark.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc06fe",
   "metadata": {},
   "source": [
    "## 1 Recommender Systems as a  Bandit problem\n",
    "\n",
    "Recommender systems are sequential decision problems since users interact with items over a period of time and reveal their preferances.The\n",
    "fundamental explore/exploit tradeoff (a feature of  multi-armed bandits/Reinforcement Learning) exists in recommender systems as the recommender\n",
    "agent must choose between recommending the highest rated item to each user as per its current state of knowledge (exploit) and recommending items\n",
    "which potentially increases the agent's knowledge (reduce uncertainty)  about the preferences of users and attributes of items (explore).The training data available to the agent to update its knowledge of users/items preferences is dependent upon the recommendations it served in the past.This is a feature of Reinforcement Learning (RL) rather than pure  Supervised Learning.A simple recommender agent interface would be as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d779afeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderInterface:\n",
    "    \"\"\"Interface for recommender agents\"\"\"\n",
    "    def get_recommendations(self, userids:np.ndarray) -> pd.DataFrame:\n",
    "        \"\"\" Perform Inference,I.E give top recommended item for each user in userids,based of current\n",
    "           user/item knowledge\"\"\"\n",
    "        pass\n",
    "\n",
    "    def learn(self, interactions:pd.DataFrame,*args,**kwargs) -> None:\n",
    "        \"\"\"Update User/Item attributes based on xplicit user feedback of recommendations made\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58613e7",
   "metadata": {},
   "source": [
    "## 2 Collaborative Filtering\n",
    "\n",
    "The Recommender Agent can learn user preferances and item attributes through multiple ways. The two popular approaches are collaborative filtering and content based filtering. In collaborative filtering (which we will be focusing on),the agent learns user preferances and item\n",
    "attributes through explicit user feedback (ratings) of the recommended items.A common approach is to use Matrix Factorization algorithms to \n",
    "find latent factors in a low dimension that represent user/item attributes,the idea is to uncover such factors which match the observed ratings\n",
    "given by the users to various recommended items.\n",
    "\n",
    "\n",
    "### 2.1 Matrix factorization for collaborative filtering problem\n",
    "\n",
    "Matrix factorization is a common technique used in recommendation tasks. Basically, a matrix factorization algorithm tries to find latent factors that represent intrinsic user and item attributes in a lower dimension. That is,\n",
    "\n",
    "$$\\hat r_{u,i} = q_{i}^{T}p_{u}$$\n",
    "\n",
    "where $\\hat r_{u,i}$ is the predicted ratings for user $u$ and item $i$, and $q_{i}^{T}$ and $p_{u}$ are latent factors for item and user, respectively. The challenge to the matrix factorization problem is to find $q_{i}^{T}$ and $p_{u}$. This is achieved by methods such as matrix decomposition. A learning approach is therefore developed to converge the decomposition results close to the observed ratings as much as possible. Furthermore, to avoid overfitting issue, the learning process is regularized. For example, a basic form of such matrix factorization algorithm is represented as below.\n",
    "\n",
    "$$\\min\\sum(r_{u,i} - q_{i}^{T}p_{u})^2 + \\lambda(||q_{i}||^2 + ||p_{u}||^2)$$\n",
    "\n",
    "where $\\lambda$ is a the regularization parameter. \n",
    "\n",
    "\n",
    "### 2.2 Alternating Least Square (ALS)\n",
    "\n",
    "Owing to the term of $q_{i}^{T}p_{u}$ the loss function is non-convex. Gradient descent method can be applied but this will incur expensive computations. An Alternating Least Square (ALS) algorithm was therefore developed to overcome this issue. \n",
    "\n",
    "The basic idea of ALS is to learn one of $q$ and $p$ at a time for optimization while keeping the other as constant. This makes the objective at each iteration convex and solvable. The alternating between $q$ and $p$ stops when there is convergence to the optimal. It is worth noting that this iterative computation can be parallelised and/or distributed, which makes the algorithm desirable for use cases where the dataset is large and thus the user-item rating matrix is super sparse (as is typical in recommendation scenarios). A comprehensive discussion of ALS and its distributed computation can be found [here](http://stanford.edu/~rezab/classes/cme323/S15/notes/lec14.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26539f7",
   "metadata": {},
   "source": [
    "## 2.3 ALS based Recommender agent\n",
    "\n",
    "Following is an implementation of a recommender agent which uses the ALS Algorithm to learn user/item latent factors ($q_{i}^{T}$ and $p_{u}$) from ratings data.Note that in order to serve recommendations at any point in time,the agent recommends the item with the highest predicted rating for each user.This implies that the agent is implementing a 'greedy' policy i.e exploiting its state of knowledge at each step,without making exploratory recommendations.This results in sub-optimal performance (as we will see later).The greedy policy can be improved by using an\n",
    "$\\epsilon$-greedy approach,i.e with a small probability $\\epsilon$ the agent will recommend a random item from the catalogue to a user and with a probability 1-$\\epsilon$ it will recommend the top rated item.This ensures continuous exploration but has the drawback that the agent continues to make sub-optimal recos even after learning a lot about user/item attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "823df050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ALSRecommender(RecommenderInterface):\n",
    "    \"\"\"Wrapper class to use ALS algorithm to fit user/item latent factors to interaction data\n",
    "       and serve top recommendations for requested userids (run inference) using model state\n",
    "       \"\"\"\n",
    "\n",
    "    def __init__(self,n_items,**kwargs):\n",
    "        \"\"\" Args:\n",
    "            als = ALS(\n",
    "            maxIter=1, : ALS iterations- kept as 1 to match bayesian recommender updates\n",
    "            rank=10,  :  number of latent factors (n_features)\n",
    "            regParam=0.05, : regularization hyperparam\n",
    "         )\n",
    "         \"\"\"\n",
    "        \n",
    "        ## store itemids\n",
    "        self.itemids=np.array(range(n_items))\n",
    "        \n",
    "        ## init spark session\n",
    "        findspark.init()\n",
    "        self.spark = start_or_get_spark(\"ALS Deep Dive\", memory=\"8g\")\n",
    "        self.spark.conf.set(\"spark.sql.analyzer.failAmbiguousSelfJoin\", \"false\")\n",
    "        \n",
    "        ## init ALS model class with hyperparams passed down from caller\n",
    "        self.als = ALS(**kwargs,\n",
    "                    userCol='userid', \n",
    "                    itemCol='itemid', \n",
    "                    ratingCol='actual_rating', \n",
    "                    coldStartStrategy='drop')\n",
    "        \n",
    "        ## init  spark schema to convert pd dfs to spark dfs\n",
    "        self.inference_schema = StructType((\n",
    "        StructField('userid', IntegerType()),\n",
    "        StructField('itemid', IntegerType()),\n",
    "           ))\n",
    "        self.learning_schema=StructType((\n",
    "        StructField('userid', IntegerType()),\n",
    "        StructField('itemid', IntegerType()),\n",
    "        StructField('actual_rating', FloatType()),\n",
    "        \n",
    "           ))\n",
    "        \n",
    "        ## since coldStartStrategy=\"drop\" for the ALS model,initially (when no user/item)\n",
    "        ## interactions are observed,model cannot predict ratings and hence random recomme-\n",
    "        ##-ndations are served.the flag is set to false as soon as ALSRecALSRecommender.learn()\n",
    "        ## is called\n",
    "        self.random_initial_recos=True\n",
    "       \n",
    "        ## since the ALS model does not support incremental training (as new interaction data\n",
    "        ## comes in) .. we append the data into an ever increasing dataset and fit on all the  \n",
    "        ## interactions available at any given time.\n",
    "        self.interaction_data=pd.DataFrame()\n",
    "        \n",
    "    def get_recommendations(self,userids:np.ndarray)->pd.DataFrame:\n",
    "        \n",
    "        if self.random_initial_recos:\n",
    "            rec_items=np.random.choice(self.itemids,size=len(userids),replace=True)\n",
    "            pred_ratings=np.NAN\n",
    "            return pd.DataFrame({'userid':userids,'itemid':rec_items,'predicted_rating':pred_ratings})\n",
    "        \n",
    "        else:\n",
    "            ## run inference on all user/item pairs and recommend the highest rated item for each user\n",
    "            user_items=pd.DataFrame({'userid':userids}).merge(pd.DataFrame({'itemid':self.itemids}),\n",
    "                                     how='cross')\n",
    "            ## convert to spark df to run inference with ALS model\n",
    "            infer_df=self.spark.createDataFrame(user_items,schema=self.inference_schema) \n",
    "            ## inference step\n",
    "            pred_df=self.model.transform(infer_df)\n",
    "            ## rename inferred (prediction) column -coming from ALS implementation to predicted_rating\n",
    "            ## to make it consistent\n",
    "            pred_df=pred_df.withColumnRenamed(\"prediction\",\"predicted_rating\")\n",
    "            ## convert predictions back to a pandas df\n",
    "            pred_df=pred_df.toPandas()\n",
    "            ## recommend top item for each user,ranked by predicted rating\n",
    "            recos=pred_df.loc[pred_df.groupby('userid')['predicted_rating'].idxmax()]\n",
    "            \n",
    "        \n",
    "            return recos\n",
    "\n",
    "            \n",
    "            \n",
    "    def learn(self,interactions:pd.DataFrame)->None:\n",
    "        ## set random recommendations flag to false when learning begins\n",
    "        self.random_initial_recos=False\n",
    "        self.interaction_data=pd.concat([self.interaction_data,interactions])\n",
    "        ## convert to spark df to fit als model\n",
    "        interactions=self.spark.createDataFrame(self.interaction_data[['userid','itemid','actual_rating']]\n",
    "                                                ,schema=self.learning_schema) \n",
    "        \n",
    "        ## fit model on all interaction data available accumalated this time\n",
    "        self.model=self.als.fit(interactions)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f660a61b",
   "metadata": {},
   "source": [
    "# 3 Bayesian Recommender agent\n",
    "\n",
    "In order to address the explore/exploit tradeoff in the recommender system setting,we propose a bayesian update strategy to update the agent's\n",
    "beliefs about user/item attributes from ratings data.The key difference here is that instead of learning user/item latent vectors ($q_{i}^{T}$ and $p_{u}$),we learn (multivariate-gaussian) distributions for each user and item, i.e  $q_{i}$ ~ N($\\mu$_i,$\\sigma$_i) and  $p_{u}$ ~ N($\\mu$_u,$\\sigma$_u).This allows us to sample user/item distributions to predict ratings and recommend the highest rated item.\n",
    "\n",
    "The main advantage of this approach is that the agent makes recommendations that either:\n",
    "\n",
    "* It is confident would result in a high rating\n",
    "* It has high uncertainity about (high variance of user/item attributes),this helps in exploring possibly better recommendations.\n",
    "\n",
    "This approach is inspired from Thompson Sampling for bernoulli bandits,which uses bayesian updates on a prior beta distribution to sample actions. More details of which can be found [here](https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa032e5",
   "metadata": {},
   "source": [
    "## 3.1 Bayesian Linear Regression\n",
    "\n",
    "The primary building block of our bayesian recommender system is the update mechanism which performs updates to user/item latent variable distributions from observed interactions i.e \n",
    "\n",
    "if we have prior beliefs about user u and item i,such that their latent attributes are distributed as:$$\\hat p_{u} ~ N(\\mu_u,\\Sigma_u)$$ and\n",
    "$$\\hat q_{i} ~ N(\\mu_i,\\Sigma_i)$$ and we observe an interaction  which results in a rating $r_{u,i}$  where $$\\hat r_{u,i} = q_{i}^{T}p_{u} + \\epsilon$$ \n",
    "\n",
    "where $\\epsilon$ is a noise term with known std. deviation = $\\beta$,then the posterior distributions of $p_{u}$ | $q_{i}$,$r_{u,i}$\n",
    "and  $q_{i}$ | $p_{u}$,$r_{u,i}$ will be gaussian too. The new means and variances $\\mu_u^*$,$\\Sigma_u^*$ and $\\mu_i^*$,$\\Sigma_i^*$ are related \n",
    "to the original - $\\mu_u$,$\\sigma_u$ and $\\mu_i$,$\\sigma_i$ as follows:\n",
    "\n",
    "## 3.2 Bayesian Update\n",
    "\n",
    "Suppose we have to update our  **user** attribute distribution given an interaction $I_{u,i}$ between user $u$ and item $i$ resulting in a rating \n",
    "$r_{u,i}$,with prior attribute distributions $ N(\\mu_i,\\Sigma_i)$ and $ N(\\mu_u,\\Sigma_u)$ respectively, first we draw a dataset $\\Phi$ comprising of n_samples of possible **item** attributes sampled from $ N(\\mu_i,\\Sigma_i)$ ,then the posterior distribution of our **user** attributes $p_{u}$|$r_{u,i}$, $\\Phi$,$\\mu_u$,$\\Sigma_u$,$\\beta$ ~ N($\\mu_u^*$,$\\Sigma_u^*$) is defined by : \n",
    "\n",
    "$$Inv(\\Sigma_u^*)=Inv(\\Sigma_u)+\\beta\\Phi^{T}\\Phi$$  and $$\\mu_u^*=\\Sigma_u^*(Inv(\\Sigma_u)\\mu_u+\\beta\\Phi^{T}r_{u,i})$$ \n",
    "\n",
    "Where $Inv()$ is the matrix inverse operator. These equations corospond to the update rules for bayesian linear regression as decribed in the \n",
    "textbook Pattern Recognition and Machine Learning by Christopher M. Bishop (eqs 3.50,3.51) as found here-http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf. \n",
    "\n",
    "After updating our beliefs about our **user** attribute distribution,we follow the same process interchanging the **user** and **item** distributions to make a similar update to our **item** distribution . Note that we use the updated **user** distribution from the last step to\n",
    "make our **item** update. This (**user** and **item**) update  completes one *iteration*\n",
    "\n",
    "A simple implementation of a distribution class that can be sampled from and can make the aforementioned Bayesian updates is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a21bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianDistribution():\n",
    "\n",
    "    def __init__(self, n_features=10, alpha=10., beta=0.1,**kwargs):\n",
    "        self.n_features = n_features\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.mean = np.zeros(n_features)\n",
    "        self.cov_inv = np.identity(n_features) / alpha\n",
    "        self.cov=np.identity(n_features) * alpha\n",
    "    \n",
    "    def sample(self,n_samples:int)->np.ndarray:\n",
    "        return np.random.multivariate_normal(mean=self.mean,cov=self.cov,size=n_samples)\n",
    "        \n",
    "    \n",
    "\n",
    "    def learn(self, X, y):\n",
    "        # If x and y are singletons, then we coerce them to a batch of length 1\n",
    "        X = np.atleast_2d(X)\n",
    "        y = np.atleast_1d(y)\n",
    "        \n",
    "      \n",
    "        # Update the inverse covariance matrix (Bishop eq. 3.51)\n",
    "        cov_inv_post = self.cov_inv + self.beta * X.T@X\n",
    "\n",
    "        # update covariance by inverting posterior cov_inv\n",
    "        self.cov = np.linalg.inv(cov_inv_post)\n",
    "         # Update the mean vector (Bishop eq. 3.50)\n",
    "      \n",
    "        mean = self.cov @ (self.cov_inv @ self.mean + self.beta *X.T@y)\n",
    "        self.cov_inv = cov_inv_post\n",
    "        self.mean = mean\n",
    "\n",
    "        \n",
    "    def predict_mean(self,X):\n",
    "        return X@self.mean\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        # Obtain the predictive mean (Bishop eq. 3.58)\n",
    "        y_pred_mean = x @ self.mean\n",
    "\n",
    "        # Obtain the predictive variance (Bishop eq. 3.59)\n",
    "        w_cov = np.linalg.inv(self.cov_inv)\n",
    "        y_pred_var = 1 / self.beta + x @ w_cov @ x.T\n",
    "\n",
    "        return stats.norm(loc=y_pred_mean, scale=y_pred_var ** .5)\n",
    "    \n",
    "    ## just so alling code can refer to wts dist as BayesianDistribution().weights_dist\n",
    "    @property\n",
    "    def weights_dist(self):\n",
    "        cov = np.linalg.inv(self.cov_inv)\n",
    "        return stats.multivariate_normal(mean=self.mean, cov=cov)\n",
    "    \n",
    " \n",
    "    def run_bayesian_reg_test(self,x_count,max_iters):\n",
    "        np.random.seed(1234)\n",
    "        dist_learner=BayesianDistribution(n_features=self.n_features,alpha=self.alpha,beta=self.beta)\n",
    "        true_wts=np.random.rand(*dist_learner.mean.shape)\n",
    "        res=[]\n",
    "        for i in range(max_iters):\n",
    "            # create random data\n",
    "            X=np.random.rand(x_count,dist_learner.n_features)\n",
    "            y=X@true_wts+np.random.randn(x_count,1)*dist_learner.beta\n",
    "            \n",
    "            # calc metrics to track\n",
    "            y_pred=dist_learner.predict_mean(X)\n",
    "            target_mse=sklearn.metrics.mean_squared_error(y,y_pred)\n",
    "            wts_mse=sklearn.metrics.mean_squared_error(true_wts,dist_learner.mean)\n",
    "            \n",
    "            row={'iteration':i,'target_MSE':target_mse,'features_MSE': wts_mse,\n",
    "                 'true_features':true_wts,'estimated_features':dist_learner.mean}\n",
    "            res.append(row)\n",
    "            \n",
    "            ## learning is inplace,modifying the prior_dist variable\n",
    "            dist_learner.learn(X,y)\n",
    "         \n",
    "        return pd.DataFrame(res)  \n",
    "            \n",
    "           \n",
    "           \n",
    "            \n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79bc7e8",
   "metadata": {},
   "source": [
    "## 3.3 Bayesian Recommender Algorithm:\n",
    "* Initialize user,item attribute distributions with uninformative priors ($\\mu$=0, $\\Sigma$=large)\n",
    "    <br>\n",
    "* for t in n_timeperiods do:\n",
    "> * Receive recommendation requests for a list of users \n",
    "> * sample n=1 samples for user,item attributes ( $p_{u}$ and $q_{i}$) for all users in requests and items in the catelogue using\n",
    "> the current user/item distributions $ N(\\mu_i,\\Sigma_i)$ and $ N(\\mu_u,\\Sigma_u)$\n",
    "> * Recommend to each user u, an item i which has the highest predicted rating i.e i=argmax($q_{i}^{T}p_{u}$)\n",
    "> * Receive ratings $r_{u,i}$ for each recommended pair user u,item i ,these form the training data or ground truth with which to update\n",
    "> user/item distributions.\n",
    "> * for each $r_{u,i}$ in ratings do:\n",
    ">> * for i in n_iterations:\n",
    ">>> * Sample **item** attributes from current item attribute distribution ($ N(\\mu_i,\\Sigma_i)$),store samples in dataset $\\Phi$\n",
    ">>> * Update **user** attribute distribution using the Bayesian Update rule decribed in sec 3.2 \n",
    "<br>\n",
    ">>> i.e $\\mu_u^*$,$\\Sigma_u^*$->BayesianUpdate($r_{u,i}$,$\\Phi$,$\\beta$,$\\mu_u$,$\\Sigma_u$)\n",
    ">>> * Sample **user** attributes from current user attribute distribution ($ N(\\mu_u^*,\\Sigma_u^*)$),store samples in dataset $\\Phi^*$\n",
    ">>> * Update **item** attribute distribution using the Bayesian Update rule decribed in sec 3.2 \n",
    "<br>\n",
    ">>> i.e $\\mu_i^*$,$\\Sigma_i^*$->BayesianUpdate($r_{u,i}$,$\\Phi^*$,$\\beta$,$\\mu_i$,$\\Sigma_i$)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0617c23c",
   "metadata": {},
   "source": [
    "A Recommender agent implementing the above algorithm would be as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b23edff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianRecommender(RecommenderInterface):\n",
    "\n",
    "    def __init__(self,n_items, dist_cls=BayesianDistribution,epsilon_greedy=False,**kwargs):\n",
    "        \"\"\"Bayesian recommender class.Internally store and update distributions over user and item\n",
    "           features(embeddings) Args:\n",
    "           \n",
    "           dist_cls:parametrized distribution class representing current belief of user/item attributes\n",
    "                    which is updatable with incoming ratings data\n",
    "           \n",
    "           dist_kwargs:kwargs to init dist_cls. represent prior belief for user/items for which no\n",
    "                       ratings data is yet available\n",
    "            \"\"\"\n",
    "        \n",
    "        init_user_item_func=partial(dist_cls,**kwargs)\n",
    "       \n",
    "        ## defaultdicts to initialise users/items which have no interaction data with uninformative priors\n",
    "        ## i.e large covariance matrices, key: user/item id ,value: instance of the distribution class\n",
    "        ##used to represent belief over user/item attributes ,given the ratings data so far. \n",
    "       \n",
    "        self.USERS=defaultdict(init_user_item_func)\n",
    "        self.ITEMS=defaultdict(init_user_item_func)\n",
    "        self.itemids=np.array(range(n_items))\n",
    "        \n",
    "        \n",
    "    def get_recommendations(self,userids:np.ndarray)->pd.DataFrame:\n",
    "        \n",
    "        ## sampling user and item attributes from respective distributions,\n",
    "        ## and then recommending item with highest predicted rating. This is akin\n",
    "        ## to thompson sampling\n",
    "        user_attrs=np.stack([self.USERS[userid].sample(1).flatten() for userid in userids],axis=1)\n",
    "        item_attrs=np.stack([self.ITEMS[itemid].sample(1).flatten() for itemid in self.itemids],axis=1)\n",
    "        \n",
    "        ## calculating expected ratings for requested userids and all items in inventory \n",
    "        ratings_mat=item_attrs.T@user_attrs\n",
    "        \n",
    "        ## recommend top item as per predicted ratings\n",
    "        rec_items=self.itemids[np.argmax(ratings_mat,axis=0)]    \n",
    "        pred_rating=np.max(ratings_mat,axis=0)\n",
    "        \n",
    "        \n",
    "        recos=pd.DataFrame({'userid':userids,'itemid':rec_items,'predicted_rating':pred_rating})\n",
    "        return recos\n",
    "\n",
    "\n",
    "    def learn(self,interactions:pd.DataFrame,n_iters=1,n_samples=1000)->None:\n",
    "        \n",
    "        ## do bayesian updates to each user/item distributions for each user/item interaction based on actual interaction data\n",
    "        for userid,itemid,target_score in zip(interactions['userid'],interactions['itemid'],interactions['actual_rating']):\n",
    "            \n",
    "            user_dist,item_dist=self.USERS[userid],self.ITEMS[itemid]\n",
    "            target_score=np.repeat(target_score,n_samples)\n",
    "            \n",
    "            for i in range(n_iters):\n",
    "                ## first update user distribution while sampling from the item dist\n",
    "                user_dist.learn(item_dist.sample(n_samples),target_score)\n",
    "               \n",
    "                ## then update item distribution while sampling from the updated user dist\n",
    "                item_dist.learn(user_dist.sample(n_samples),target_score)\n",
    "                ## both updates increase the mean likelyhood of the interaction data given\n",
    "                ## user/item beliefs.The likelyhood is upper bounded by the inner product\n",
    "                ## of the true user-item attributes,hence with enough interactions the\n",
    "                ## mean of the user/item distributions should converge to the true attribute\n",
    "                ## vector for all users and items\n",
    "               \n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef6688f",
   "metadata": {},
   "source": [
    "# 4 Experiment Design\n",
    "\n",
    "In order to evaluate our recommender agents in the sequential setting (when users preferences and item attributes are revealed over time),we use\n",
    "a user/item simulator instead of a static dataset of user/item IDs and corresponding ratings.This allows us to not only compute standard evaluation metrics (like M.S.E error) between predicted and actual ratings, but also the **Regret** which is the difference in ratings between a the highest rated item and the recommended item for any user. A decrease in **Regret** would imply that the recommender was making better recommendations\n",
    "\n",
    "## 4.1 User/Item Simulator\n",
    "\n",
    "The User/Item simulator contains the **TRUE** user and item attributes ($Q_{i}$ and $P_{i}$),then the **TRUE** assigned rating $r_{u,i}$|$Q_{i}$,$P_{i}$\n",
    "is given by\n",
    "$$\\ r_{u,i} = Q_{i}^{T}P_{u} + \\epsilon$$ \n",
    "where $\\epsilon$ is the ratings noise. The **TRUE** attributes $Q_{i}$ and $P_{i}$ are initialized randomly to values between 0 and 1 and are hidden from the recommender agent.The agent must infer these attributes by using the ratings generated by the simulator for its recommendations.\n",
    "Below is an implementation for such a user/Item Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "951c8a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserItemSimulator():\n",
    "\n",
    "    def __init__(self, n_users=100000,n_items=1000,n_features=10,rat_noise=0.1):\n",
    "        \"\"\"Class to simulate actual user/item interactions and generate recommendation requests Args:\n",
    "           \n",
    "           n_users:max number of users interacting with the system\n",
    "           \n",
    "           user_pct: %tage of users which request recommendation in one time period\n",
    "           \n",
    "           n_items: number of items in recommendation inventory\n",
    "           \n",
    "           rat_noise:s.d. of the noise in the expected rating,given user/item attrs\n",
    "           \n",
    "           i.e true_rating=user_attrs@item_attrs.T+noise\n",
    "            \"\"\"\n",
    "    \n",
    "        ## storing true user/item attrs in np.arrays of shape (n_users/items,n_features)\n",
    "        ## this way they can be integer indexed as user_attrs=self.USER_ATTRS[userid] etc.\n",
    "        \n",
    "        self.userids=np.array(range(n_users))\n",
    "        self.USER_ATTRS=np.random.rand(n_users,n_features)\n",
    "        self.ITEM_ATTRS=np.random.rand(n_items,n_features)\n",
    "        self.rating_noise=rat_noise\n",
    "        \n",
    "        ## precalculating true mean true ratings for all users and items\n",
    "        self.TRUE_RATINGS_MAT=self.ITEM_ATTRS@self.USER_ATTRS.T\n",
    "        \n",
    "        ## precalculating best possible ratings for  optimal recommendations\n",
    "        self.BEST_RECOS=np.max(self.TRUE_RATINGS_MAT,axis=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def generate_requests(self,n_requests=10000):\n",
    "        return np.random.choice(self.userids,size=n_requests,replace=False)\n",
    "    \n",
    "    def score_reccomendations(self,recos):\n",
    "        recos['actual_rating_mean']=self.TRUE_RATINGS_MAT[recos['itemid'],recos['userid']]\n",
    "        ## the actual ratings have a noise component to them,added to the mean...these are the ratings we use to learn\n",
    "        recos['actual_rating']=  recos['actual_rating_mean']  +self.rating_noise*np.random.randn()\n",
    "        recos['best_rating_mean']=self.BEST_RECOS[recos['userid']]\n",
    "        ## use the mean actual ratings to compute regret so regret goes to 0 when we select the items with the highest average rating for each user \n",
    "        recos['regret']=recos['best_rating_mean']-recos['actual_rating_mean']\n",
    "        return recos\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee14d189",
   "metadata": {},
   "source": [
    "## 4.2 Usage/Demo\n",
    "\n",
    "### Init user/item simulator and Recommender agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52e1e779",
   "metadata": {},
   "outputs": [],
   "source": [
    "## initing user-item simulator\n",
    "user_sim=UserItemSimulator(n_users=1000,n_items=1000,n_features=10,rat_noise=0.1)\n",
    "## init bayesian recommender\n",
    "b_rec=BayesianRecommender(n_items=1000,n_features=10, alpha=10., beta=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f004bdeb",
   "metadata": {},
   "source": [
    "### Get Recommendation Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4df049db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     userid\n",
       "0       597\n",
       "1       176\n",
       "2       606\n",
       "3       705\n",
       "4       771\n",
       "..      ...\n",
       "995      92\n",
       "996     179\n",
       "997     445\n",
       "998     198\n",
       "999     186\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests=user_sim.generate_requests(1000)\n",
    "pd.DataFrame({'userid':requests})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f905343",
   "metadata": {},
   "source": [
    "### Get recommendations for the user requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a575b873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>itemid</th>\n",
       "      <th>predicted_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>597</td>\n",
       "      <td>469</td>\n",
       "      <td>130.524879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>176</td>\n",
       "      <td>541</td>\n",
       "      <td>38.697143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>606</td>\n",
       "      <td>805</td>\n",
       "      <td>71.775643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>705</td>\n",
       "      <td>810</td>\n",
       "      <td>92.562352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>771</td>\n",
       "      <td>588</td>\n",
       "      <td>88.547633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>92</td>\n",
       "      <td>547</td>\n",
       "      <td>49.742615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>179</td>\n",
       "      <td>621</td>\n",
       "      <td>135.450418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>445</td>\n",
       "      <td>124</td>\n",
       "      <td>105.282972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>198</td>\n",
       "      <td>84</td>\n",
       "      <td>108.077796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>186</td>\n",
       "      <td>829</td>\n",
       "      <td>120.707429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     userid  itemid  predicted_rating\n",
       "0       597     469        130.524879\n",
       "1       176     541         38.697143\n",
       "2       606     805         71.775643\n",
       "3       705     810         92.562352\n",
       "4       771     588         88.547633\n",
       "..      ...     ...               ...\n",
       "995      92     547         49.742615\n",
       "996     179     621        135.450418\n",
       "997     445     124        105.282972\n",
       "998     198      84        108.077796\n",
       "999     186     829        120.707429\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recos=b_rec.get_recommendations(requests)\n",
    "recos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fd1bd5",
   "metadata": {},
   "source": [
    "### Score generated recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35ac71ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>itemid</th>\n",
       "      <th>predicted_rating</th>\n",
       "      <th>actual_rating_mean</th>\n",
       "      <th>actual_rating</th>\n",
       "      <th>best_rating_mean</th>\n",
       "      <th>regret</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>237</td>\n",
       "      <td>291</td>\n",
       "      <td>102.563792</td>\n",
       "      <td>3.649996</td>\n",
       "      <td>3.642422</td>\n",
       "      <td>4.476816</td>\n",
       "      <td>0.826820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>914</td>\n",
       "      <td>381</td>\n",
       "      <td>96.551606</td>\n",
       "      <td>2.485622</td>\n",
       "      <td>2.478049</td>\n",
       "      <td>4.539948</td>\n",
       "      <td>2.054326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>484</td>\n",
       "      <td>446</td>\n",
       "      <td>119.845155</td>\n",
       "      <td>1.521432</td>\n",
       "      <td>1.513858</td>\n",
       "      <td>3.541389</td>\n",
       "      <td>2.019958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>657</td>\n",
       "      <td>799</td>\n",
       "      <td>72.066049</td>\n",
       "      <td>1.523088</td>\n",
       "      <td>1.515515</td>\n",
       "      <td>3.274659</td>\n",
       "      <td>1.751570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>144</td>\n",
       "      <td>839</td>\n",
       "      <td>59.076952</td>\n",
       "      <td>2.222614</td>\n",
       "      <td>2.215041</td>\n",
       "      <td>3.600775</td>\n",
       "      <td>1.378160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>414</td>\n",
       "      <td>258</td>\n",
       "      <td>81.044592</td>\n",
       "      <td>2.944138</td>\n",
       "      <td>2.936564</td>\n",
       "      <td>4.770941</td>\n",
       "      <td>1.826804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>131</td>\n",
       "      <td>306</td>\n",
       "      <td>129.513825</td>\n",
       "      <td>1.271351</td>\n",
       "      <td>1.263777</td>\n",
       "      <td>3.813208</td>\n",
       "      <td>2.541857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>998</td>\n",
       "      <td>787</td>\n",
       "      <td>85.131933</td>\n",
       "      <td>2.201177</td>\n",
       "      <td>2.193603</td>\n",
       "      <td>3.964226</td>\n",
       "      <td>1.763049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>604</td>\n",
       "      <td>242</td>\n",
       "      <td>102.116459</td>\n",
       "      <td>3.319046</td>\n",
       "      <td>3.311472</td>\n",
       "      <td>4.961711</td>\n",
       "      <td>1.642665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>983</td>\n",
       "      <td>661</td>\n",
       "      <td>75.528392</td>\n",
       "      <td>2.723942</td>\n",
       "      <td>2.716368</td>\n",
       "      <td>4.788221</td>\n",
       "      <td>2.064279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     userid  itemid  predicted_rating  actual_rating_mean  actual_rating  \\\n",
       "0       237     291        102.563792            3.649996       3.642422   \n",
       "1       914     381         96.551606            2.485622       2.478049   \n",
       "2       484     446        119.845155            1.521432       1.513858   \n",
       "3       657     799         72.066049            1.523088       1.515515   \n",
       "4       144     839         59.076952            2.222614       2.215041   \n",
       "..      ...     ...               ...                 ...            ...   \n",
       "995     414     258         81.044592            2.944138       2.936564   \n",
       "996     131     306        129.513825            1.271351       1.263777   \n",
       "997     998     787         85.131933            2.201177       2.193603   \n",
       "998     604     242        102.116459            3.319046       3.311472   \n",
       "999     983     661         75.528392            2.723942       2.716368   \n",
       "\n",
       "     best_rating_mean    regret  \n",
       "0            4.476816  0.826820  \n",
       "1            4.539948  2.054326  \n",
       "2            3.541389  2.019958  \n",
       "3            3.274659  1.751570  \n",
       "4            3.600775  1.378160  \n",
       "..                ...       ...  \n",
       "995          4.770941  1.826804  \n",
       "996          3.813208  2.541857  \n",
       "997          3.964226  1.763049  \n",
       "998          4.961711  1.642665  \n",
       "999          4.788221  2.064279  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scored_interactions=user_sim.score_reccomendations(recos)\n",
    "scored_interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fe4e5f",
   "metadata": {},
   "source": [
    "### Improve Recommender agent with the scored interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44773dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_rec.learn(scored_interactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d883993c",
   "metadata": {},
   "source": [
    "## Experiment Flowchart\n",
    "![Experiment Flowchart](Recommender_expt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86888e6b",
   "metadata": {},
   "source": [
    "## Helper Class to time code run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0620ecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer\n",
    "\n",
    "\n",
    "class Timer():\n",
    "    def __init__(self, verbose=False):\n",
    "        self.verbose = verbose\n",
    "        self.timer = default_timer\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.start = self.timer()\n",
    "        return self\n",
    "        \n",
    "    def __exit__(self, *args):\n",
    "        end = self.timer()\n",
    "        self.elapsed = end - self.start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc940088",
   "metadata": {},
   "source": [
    "## 4.3 Final Experiment \n",
    "Below is the code for running the experiment for a specified number of periods and specified number of users and items (n_periods=30,50 etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "709bc66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_expt(n_periods,n_users=10000,n_items=100):\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    ## initing user-item simulator\n",
    "    user_sim=UserItemSimulator(n_users,n_items,n_features=10,rat_noise=0.1)\n",
    "    ## init bayesian recommender\n",
    "    b_rec=BayesianRecommender(n_items=n_items,n_features=10, alpha=10., beta=0.1)\n",
    "    ## init ALS recommender\n",
    "    a_rec=ALSRecommender(maxIter=1,rank=10,regParam=0.05,n_items=n_items)\n",
    "    ## run recommend-score-update loop for n_periods\n",
    "    log=[]\n",
    "    for t in tqdm(range(n_periods)):\n",
    "        ## generate recommendation requests\n",
    "        requests=user_sim.generate_requests(n_users)\n",
    "        ## get recommendations\n",
    "        with Timer() as infer_timer_a:\n",
    "            recos_a=a_rec.get_recommendations(requests)\n",
    "            \n",
    "        with Timer() as infer_timer_b:\n",
    "            recos_b=b_rec.get_recommendations(requests)\n",
    "            \n",
    "        ## score recommendations\n",
    "        interactions_a=user_sim.score_reccomendations(recos_a)\n",
    "        interactions_b=user_sim.score_reccomendations(recos_b)\n",
    "        \n",
    "        ## update/fit recommenders with iteraction data\n",
    "        with Timer() as train_timer_a:\n",
    "            a_rec.learn(interactions_a)\n",
    "            \n",
    "        with Timer() as train_timer_b:\n",
    "            b_rec.learn(interactions_b)\n",
    "            \n",
    "        ## insert scoring/mse error/regret tracking code here\n",
    "        row={'mean_regret_ALS':interactions_a['regret'].mean(),\n",
    "             'mean_regret_Bayesian':interactions_b['regret'].mean(),\n",
    "             'ratings_mse_ALS':((interactions_a['predicted_rating']-interactions_a['actual_rating_mean'])**2).mean(),\n",
    "             'ratings_mse_Bayesian':((interactions_b['predicted_rating']-interactions_b['actual_rating_mean'])**2).mean(),\n",
    "             'inferance_time_ALS':infer_timer_a.elapsed,\n",
    "            'inferance_time_Bayesian':infer_timer_b.elapsed,\n",
    "             'training_time_ALS':train_timer_a.elapsed,\n",
    "             'training_time_Bayesian':train_timer_b.elapsed\n",
    "            }\n",
    "        log.append(row)\n",
    "        clear_output(wait=True)\n",
    "        display(pd.DataFrame(log))\n",
    "        \n",
    "    \n",
    "    return pd.DataFrame(log),user_sim,a_rec,b_rec\n",
    "        \n",
    "    \n",
    "    \n",
    "                              \n",
    "        \n",
    "    \n",
    "                              \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50078bee",
   "metadata": {},
   "source": [
    "## Run and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b1c471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_regret_ALS</th>\n",
       "      <th>mean_regret_Bayesian</th>\n",
       "      <th>ratings_mse_ALS</th>\n",
       "      <th>ratings_mse_Bayesian</th>\n",
       "      <th>inferance_time_ALS</th>\n",
       "      <th>inferance_time_Bayesian</th>\n",
       "      <th>training_time_ALS</th>\n",
       "      <th>training_time_Bayesian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.583331</td>\n",
       "      <td>1.568143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10104.148497</td>\n",
       "      <td>0.021003</td>\n",
       "      <td>1.113949</td>\n",
       "      <td>21.986770</td>\n",
       "      <td>3.406009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.215116</td>\n",
       "      <td>1.201606</td>\n",
       "      <td>0.972556</td>\n",
       "      <td>2366.448140</td>\n",
       "      <td>40.985261</td>\n",
       "      <td>0.718517</td>\n",
       "      <td>7.316114</td>\n",
       "      <td>3.319364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.180599</td>\n",
       "      <td>1.173342</td>\n",
       "      <td>0.215609</td>\n",
       "      <td>202.964224</td>\n",
       "      <td>40.007753</td>\n",
       "      <td>0.782418</td>\n",
       "      <td>6.447025</td>\n",
       "      <td>3.356611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.101050</td>\n",
       "      <td>1.153942</td>\n",
       "      <td>0.087176</td>\n",
       "      <td>94.926841</td>\n",
       "      <td>37.693448</td>\n",
       "      <td>0.909922</td>\n",
       "      <td>6.518701</td>\n",
       "      <td>3.310713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.082075</td>\n",
       "      <td>1.133270</td>\n",
       "      <td>0.063502</td>\n",
       "      <td>40.979157</td>\n",
       "      <td>38.159967</td>\n",
       "      <td>0.754672</td>\n",
       "      <td>6.906252</td>\n",
       "      <td>3.140443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_regret_ALS  mean_regret_Bayesian  ratings_mse_ALS  \\\n",
       "0         1.583331              1.568143              NaN   \n",
       "1         1.215116              1.201606         0.972556   \n",
       "2         1.180599              1.173342         0.215609   \n",
       "3         1.101050              1.153942         0.087176   \n",
       "4         1.082075              1.133270         0.063502   \n",
       "\n",
       "   ratings_mse_Bayesian  inferance_time_ALS  inferance_time_Bayesian  \\\n",
       "0          10104.148497            0.021003                 1.113949   \n",
       "1           2366.448140           40.985261                 0.718517   \n",
       "2            202.964224           40.007753                 0.782418   \n",
       "3             94.926841           37.693448                 0.909922   \n",
       "4             40.979157           38.159967                 0.754672   \n",
       "\n",
       "   training_time_ALS  training_time_Bayesian  \n",
       "0          21.986770                3.406009  \n",
       "1           7.316114                3.319364  \n",
       "2           6.447025                3.356611  \n",
       "3           6.518701                3.310713  \n",
       "4           6.906252                3.140443  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████▊                                                                     | 5/30 [03:47<19:52, 47.70s/it]"
     ]
    }
   ],
   "source": [
    "log,sim,a_rec,b_rec=run_expt(30,n_users=1000,n_items=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abdc11e",
   "metadata": {},
   "source": [
    "# 5 Results and Evaluation Metrics\n",
    "## 5.1 Regret vs MSE\n",
    "\n",
    "It is often noted that the mean square error (MSE) between predicted and actual user/item ratings ($\\hat r_{u,i}$ and  $\\ r_{u,i}$) is not \n",
    "a good metric to evaluate recommender systems. This is because the objective isn't to predict ratings with high accuracy,rather the objective\n",
    "of a recommender system is to ensure that over time users are recommended items which are likely to be assigned high ratings,eventually recommending the highest rated item in the catelogue for each user. To track this objective, we use another metric taken from RL/Bandit literature called **Regret**. The **Regret** of any action (here,recommendation) is the difference in ratings of the best possible item \n",
    "and the recommended item for any user. A **Regret**  of 0 would imply the recommender is making the best possible recommendation for a given\n",
    "user.\n",
    "\n",
    "\n",
    "## 5.2 Explore/Exploit Tradeoff\n",
    "\n",
    "In the results of our experiment run we can make the following observations:\n",
    "\n",
    "* The **Regret**  for the ALS agent drops faster initially than the Bayesaian agent,however\n",
    "  it eventually plateaus off and the Bayesian agent is able to reduce the mean **Regret** further.\n",
    "\n",
    "* The **MSE**  for The ALS agent\n",
    "    starts with a smaller initial value and quickly reduces it to near 0, whereas the Bayesian agent starts with a very high **MSE**,eventually    reducing it as it learns about the rating values.\n",
    "\n",
    "The explanation for the observed behaviour is as follows:\n",
    "\n",
    "* The ALS agent quickly overfits to the initial batches of ratings data,reducing **MSE** and becomes highly confident of its ratings\n",
    "predictions too early as a result it eventually ends up only exploiting its state of knowledge and does not explore potentially better \n",
    "recommendations resulting in a plateued performance.\n",
    "\n",
    "* The Bayesian agent starts off with a very high **MSE** (predicting ratings outside the possible range of values),this is because the user/item\n",
    "attributes are being sampled from a high variance distribution.The variance of the user/item attribute distributions are only reduced after the\n",
    "agent has tried many combinations users and items.This principle is called **optimism in the face of uncertainity** in Bandit literature and it ensures the recommender agent tries out various items for each user before converging on the optimal recommendation.At the same time the mean\n",
    "of the user/item distributions are adjusted so the selection of higher rated items becomes increasingly likely.Thus the Bayesian agent is able \n",
    "to address the Explore/Exploit tradeoff in a more effective manner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f394b4e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_regret_ALS</th>\n",
       "      <th>mean_regret_Bayesian</th>\n",
       "      <th>ratings_mse_ALS</th>\n",
       "      <th>ratings_mse_Bayesian</th>\n",
       "      <th>inferance_time_ALS</th>\n",
       "      <th>inferance_time_Bayesian</th>\n",
       "      <th>training_time_ALS</th>\n",
       "      <th>training_time_Bayesian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.108242</td>\n",
       "      <td>0.995239</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5605.170497</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.131320</td>\n",
       "      <td>4.558881</td>\n",
       "      <td>1.282589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.916315</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>0.125324</td>\n",
       "      <td>12126.136869</td>\n",
       "      <td>5.300142</td>\n",
       "      <td>0.114389</td>\n",
       "      <td>4.785893</td>\n",
       "      <td>1.175667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.899511</td>\n",
       "      <td>0.937947</td>\n",
       "      <td>0.029722</td>\n",
       "      <td>5870.117462</td>\n",
       "      <td>5.349249</td>\n",
       "      <td>0.108105</td>\n",
       "      <td>4.684738</td>\n",
       "      <td>1.153631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.893283</td>\n",
       "      <td>0.982529</td>\n",
       "      <td>0.010572</td>\n",
       "      <td>4204.639763</td>\n",
       "      <td>5.250036</td>\n",
       "      <td>0.100655</td>\n",
       "      <td>4.691514</td>\n",
       "      <td>1.176227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.887859</td>\n",
       "      <td>0.960208</td>\n",
       "      <td>0.003225</td>\n",
       "      <td>3207.297049</td>\n",
       "      <td>5.415202</td>\n",
       "      <td>0.105176</td>\n",
       "      <td>4.662004</td>\n",
       "      <td>1.154958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.888595</td>\n",
       "      <td>0.931749</td>\n",
       "      <td>0.004195</td>\n",
       "      <td>2197.325646</td>\n",
       "      <td>5.258526</td>\n",
       "      <td>0.108297</td>\n",
       "      <td>4.737060</td>\n",
       "      <td>1.134376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.887307</td>\n",
       "      <td>0.929820</td>\n",
       "      <td>0.005367</td>\n",
       "      <td>1579.098645</td>\n",
       "      <td>5.407707</td>\n",
       "      <td>0.101874</td>\n",
       "      <td>4.679289</td>\n",
       "      <td>1.174039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.886859</td>\n",
       "      <td>0.911312</td>\n",
       "      <td>0.003748</td>\n",
       "      <td>1168.826586</td>\n",
       "      <td>5.312556</td>\n",
       "      <td>0.111625</td>\n",
       "      <td>4.682302</td>\n",
       "      <td>1.204642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.887453</td>\n",
       "      <td>0.928927</td>\n",
       "      <td>0.004178</td>\n",
       "      <td>653.269551</td>\n",
       "      <td>5.339012</td>\n",
       "      <td>0.098653</td>\n",
       "      <td>4.704821</td>\n",
       "      <td>1.199471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.885962</td>\n",
       "      <td>0.923586</td>\n",
       "      <td>0.002059</td>\n",
       "      <td>343.105181</td>\n",
       "      <td>5.440180</td>\n",
       "      <td>0.104991</td>\n",
       "      <td>4.701531</td>\n",
       "      <td>1.142707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.885912</td>\n",
       "      <td>0.945865</td>\n",
       "      <td>0.002003</td>\n",
       "      <td>166.613776</td>\n",
       "      <td>5.283565</td>\n",
       "      <td>0.113756</td>\n",
       "      <td>4.724844</td>\n",
       "      <td>1.200420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.885937</td>\n",
       "      <td>0.805697</td>\n",
       "      <td>0.001888</td>\n",
       "      <td>2.728509</td>\n",
       "      <td>5.362180</td>\n",
       "      <td>0.102568</td>\n",
       "      <td>4.696148</td>\n",
       "      <td>1.140627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.886228</td>\n",
       "      <td>0.670981</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.807392</td>\n",
       "      <td>5.386128</td>\n",
       "      <td>0.099473</td>\n",
       "      <td>4.687505</td>\n",
       "      <td>1.203581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.885937</td>\n",
       "      <td>0.626503</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.342348</td>\n",
       "      <td>5.307347</td>\n",
       "      <td>0.103287</td>\n",
       "      <td>4.689091</td>\n",
       "      <td>1.185803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.885937</td>\n",
       "      <td>0.582995</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.220544</td>\n",
       "      <td>5.395019</td>\n",
       "      <td>0.109330</td>\n",
       "      <td>4.759310</td>\n",
       "      <td>1.164771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.886281</td>\n",
       "      <td>0.542717</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.147143</td>\n",
       "      <td>5.493188</td>\n",
       "      <td>0.124355</td>\n",
       "      <td>4.746042</td>\n",
       "      <td>1.224946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.885937</td>\n",
       "      <td>0.524453</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.112959</td>\n",
       "      <td>5.264203</td>\n",
       "      <td>0.125100</td>\n",
       "      <td>4.791874</td>\n",
       "      <td>1.223323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.886228</td>\n",
       "      <td>0.497359</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>0.068349</td>\n",
       "      <td>5.384911</td>\n",
       "      <td>0.102051</td>\n",
       "      <td>4.781446</td>\n",
       "      <td>1.223108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.885809</td>\n",
       "      <td>0.471510</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.062926</td>\n",
       "      <td>5.384510</td>\n",
       "      <td>0.113235</td>\n",
       "      <td>4.768883</td>\n",
       "      <td>1.195356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.885809</td>\n",
       "      <td>0.487273</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.076427</td>\n",
       "      <td>5.342298</td>\n",
       "      <td>0.107311</td>\n",
       "      <td>4.830125</td>\n",
       "      <td>1.188057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.885809</td>\n",
       "      <td>0.474640</td>\n",
       "      <td>0.001260</td>\n",
       "      <td>0.050193</td>\n",
       "      <td>5.406893</td>\n",
       "      <td>0.100891</td>\n",
       "      <td>4.860843</td>\n",
       "      <td>1.158081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.885809</td>\n",
       "      <td>0.460270</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>0.043291</td>\n",
       "      <td>5.308814</td>\n",
       "      <td>0.111063</td>\n",
       "      <td>4.792480</td>\n",
       "      <td>1.214910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.885809</td>\n",
       "      <td>0.452589</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.042065</td>\n",
       "      <td>5.301239</td>\n",
       "      <td>0.118073</td>\n",
       "      <td>4.865003</td>\n",
       "      <td>1.205305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.885809</td>\n",
       "      <td>0.444250</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.034484</td>\n",
       "      <td>5.517565</td>\n",
       "      <td>0.102825</td>\n",
       "      <td>4.980561</td>\n",
       "      <td>1.211674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.885809</td>\n",
       "      <td>0.454426</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.044436</td>\n",
       "      <td>5.447720</td>\n",
       "      <td>0.121886</td>\n",
       "      <td>4.885733</td>\n",
       "      <td>1.147498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.885809</td>\n",
       "      <td>0.446552</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.039108</td>\n",
       "      <td>5.419865</td>\n",
       "      <td>0.105182</td>\n",
       "      <td>4.925035</td>\n",
       "      <td>1.191547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.885809</td>\n",
       "      <td>0.449516</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.047931</td>\n",
       "      <td>5.464250</td>\n",
       "      <td>0.109447</td>\n",
       "      <td>4.830178</td>\n",
       "      <td>1.161599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.885809</td>\n",
       "      <td>0.448382</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.064552</td>\n",
       "      <td>5.363120</td>\n",
       "      <td>0.102492</td>\n",
       "      <td>4.916381</td>\n",
       "      <td>1.203964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.885809</td>\n",
       "      <td>0.431625</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.028789</td>\n",
       "      <td>5.328347</td>\n",
       "      <td>0.102987</td>\n",
       "      <td>4.876835</td>\n",
       "      <td>1.290904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.885809</td>\n",
       "      <td>0.425316</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.024720</td>\n",
       "      <td>5.620069</td>\n",
       "      <td>0.120170</td>\n",
       "      <td>4.902338</td>\n",
       "      <td>1.144906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.885809</td>\n",
       "      <td>0.428225</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.031145</td>\n",
       "      <td>5.381082</td>\n",
       "      <td>0.101511</td>\n",
       "      <td>4.976239</td>\n",
       "      <td>1.159897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.885809</td>\n",
       "      <td>0.421753</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.021168</td>\n",
       "      <td>5.398140</td>\n",
       "      <td>0.108424</td>\n",
       "      <td>4.885539</td>\n",
       "      <td>1.149727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.885809</td>\n",
       "      <td>0.416210</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.015180</td>\n",
       "      <td>5.389597</td>\n",
       "      <td>0.101628</td>\n",
       "      <td>4.904422</td>\n",
       "      <td>1.270199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.885809</td>\n",
       "      <td>0.427910</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.028008</td>\n",
       "      <td>5.508913</td>\n",
       "      <td>0.103480</td>\n",
       "      <td>4.866037</td>\n",
       "      <td>1.220173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.885809</td>\n",
       "      <td>0.416968</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.026729</td>\n",
       "      <td>5.389274</td>\n",
       "      <td>0.103658</td>\n",
       "      <td>4.969409</td>\n",
       "      <td>1.184749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.885809</td>\n",
       "      <td>0.418180</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.016731</td>\n",
       "      <td>5.542999</td>\n",
       "      <td>0.105630</td>\n",
       "      <td>4.983228</td>\n",
       "      <td>1.448630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.885809</td>\n",
       "      <td>0.413562</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>5.600379</td>\n",
       "      <td>0.112655</td>\n",
       "      <td>5.028212</td>\n",
       "      <td>1.162985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.885809</td>\n",
       "      <td>0.421611</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.026663</td>\n",
       "      <td>5.493910</td>\n",
       "      <td>0.107017</td>\n",
       "      <td>5.046439</td>\n",
       "      <td>1.175834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.885809</td>\n",
       "      <td>0.408856</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.006698</td>\n",
       "      <td>5.502132</td>\n",
       "      <td>0.103188</td>\n",
       "      <td>4.993618</td>\n",
       "      <td>1.301081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.885809</td>\n",
       "      <td>0.412362</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.018586</td>\n",
       "      <td>5.399477</td>\n",
       "      <td>0.106888</td>\n",
       "      <td>5.017410</td>\n",
       "      <td>1.271367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.886050</td>\n",
       "      <td>0.417559</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.026474</td>\n",
       "      <td>6.004318</td>\n",
       "      <td>0.101703</td>\n",
       "      <td>5.081787</td>\n",
       "      <td>1.188525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.885809</td>\n",
       "      <td>0.407532</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.008303</td>\n",
       "      <td>5.316276</td>\n",
       "      <td>0.112128</td>\n",
       "      <td>4.949132</td>\n",
       "      <td>1.228193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.885809</td>\n",
       "      <td>0.409039</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.008765</td>\n",
       "      <td>5.396958</td>\n",
       "      <td>0.110426</td>\n",
       "      <td>5.011008</td>\n",
       "      <td>1.218069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.885809</td>\n",
       "      <td>0.406991</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.009433</td>\n",
       "      <td>5.490754</td>\n",
       "      <td>0.114794</td>\n",
       "      <td>4.937163</td>\n",
       "      <td>1.191775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.885809</td>\n",
       "      <td>0.404709</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.006543</td>\n",
       "      <td>5.545204</td>\n",
       "      <td>0.110141</td>\n",
       "      <td>5.014268</td>\n",
       "      <td>1.141295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.885818</td>\n",
       "      <td>0.409698</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.009952</td>\n",
       "      <td>5.426411</td>\n",
       "      <td>0.102252</td>\n",
       "      <td>5.044679</td>\n",
       "      <td>1.198006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.885818</td>\n",
       "      <td>0.406082</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.009078</td>\n",
       "      <td>5.445199</td>\n",
       "      <td>0.107120</td>\n",
       "      <td>5.025216</td>\n",
       "      <td>1.186584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.885818</td>\n",
       "      <td>0.403335</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.008176</td>\n",
       "      <td>5.392847</td>\n",
       "      <td>0.101435</td>\n",
       "      <td>5.065249</td>\n",
       "      <td>1.222195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.885818</td>\n",
       "      <td>0.400592</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.006031</td>\n",
       "      <td>5.482871</td>\n",
       "      <td>0.104584</td>\n",
       "      <td>6.359135</td>\n",
       "      <td>1.797276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.885818</td>\n",
       "      <td>0.405023</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.012949</td>\n",
       "      <td>6.067215</td>\n",
       "      <td>0.128765</td>\n",
       "      <td>5.065703</td>\n",
       "      <td>1.208719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_regret_ALS  mean_regret_Bayesian  ratings_mse_ALS  \\\n",
       "0          1.108242              0.995239              NaN   \n",
       "1          0.916315              0.950820         0.125324   \n",
       "2          0.899511              0.937947         0.029722   \n",
       "3          0.893283              0.982529         0.010572   \n",
       "4          0.887859              0.960208         0.003225   \n",
       "5          0.888595              0.931749         0.004195   \n",
       "6          0.887307              0.929820         0.005367   \n",
       "7          0.886859              0.911312         0.003748   \n",
       "8          0.887453              0.928927         0.004178   \n",
       "9          0.885962              0.923586         0.002059   \n",
       "10         0.885912              0.945865         0.002003   \n",
       "11         0.885937              0.805697         0.001888   \n",
       "12         0.886228              0.670981         0.001122   \n",
       "13         0.885937              0.626503         0.000742   \n",
       "14         0.885937              0.582995         0.000712   \n",
       "15         0.886281              0.542717         0.000633   \n",
       "16         0.885937              0.524453         0.000657   \n",
       "17         0.886228              0.497359         0.000896   \n",
       "18         0.885809              0.471510         0.000895   \n",
       "19         0.885809              0.487273         0.000705   \n",
       "20         0.885809              0.474640         0.001260   \n",
       "21         0.885809              0.460270         0.001275   \n",
       "22         0.885809              0.452589         0.000642   \n",
       "23         0.885809              0.444250         0.000909   \n",
       "24         0.885809              0.454426         0.000955   \n",
       "25         0.885809              0.446552         0.000436   \n",
       "26         0.885809              0.449516         0.000291   \n",
       "27         0.885809              0.448382         0.000248   \n",
       "28         0.885809              0.431625         0.000268   \n",
       "29         0.885809              0.425316         0.000118   \n",
       "30         0.885809              0.428225         0.000080   \n",
       "31         0.885809              0.421753         0.000080   \n",
       "32         0.885809              0.416210         0.000068   \n",
       "33         0.885809              0.427910         0.000065   \n",
       "34         0.885809              0.416968         0.000102   \n",
       "35         0.885809              0.418180         0.000088   \n",
       "36         0.885809              0.413562         0.000079   \n",
       "37         0.885809              0.421611         0.000142   \n",
       "38         0.885809              0.408856         0.000152   \n",
       "39         0.885809              0.412362         0.000151   \n",
       "40         0.886050              0.417559         0.000254   \n",
       "41         0.885809              0.407532         0.000190   \n",
       "42         0.885809              0.409039         0.000119   \n",
       "43         0.885809              0.406991         0.000095   \n",
       "44         0.885809              0.404709         0.000074   \n",
       "45         0.885818              0.409698         0.000079   \n",
       "46         0.885818              0.406082         0.000073   \n",
       "47         0.885818              0.403335         0.000071   \n",
       "48         0.885818              0.400592         0.000043   \n",
       "49         0.885818              0.405023         0.000018   \n",
       "\n",
       "    ratings_mse_Bayesian  inferance_time_ALS  inferance_time_Bayesian  \\\n",
       "0            5605.170497            0.000967                 0.131320   \n",
       "1           12126.136869            5.300142                 0.114389   \n",
       "2            5870.117462            5.349249                 0.108105   \n",
       "3            4204.639763            5.250036                 0.100655   \n",
       "4            3207.297049            5.415202                 0.105176   \n",
       "5            2197.325646            5.258526                 0.108297   \n",
       "6            1579.098645            5.407707                 0.101874   \n",
       "7            1168.826586            5.312556                 0.111625   \n",
       "8             653.269551            5.339012                 0.098653   \n",
       "9             343.105181            5.440180                 0.104991   \n",
       "10            166.613776            5.283565                 0.113756   \n",
       "11              2.728509            5.362180                 0.102568   \n",
       "12              0.807392            5.386128                 0.099473   \n",
       "13              0.342348            5.307347                 0.103287   \n",
       "14              0.220544            5.395019                 0.109330   \n",
       "15              0.147143            5.493188                 0.124355   \n",
       "16              0.112959            5.264203                 0.125100   \n",
       "17              0.068349            5.384911                 0.102051   \n",
       "18              0.062926            5.384510                 0.113235   \n",
       "19              0.076427            5.342298                 0.107311   \n",
       "20              0.050193            5.406893                 0.100891   \n",
       "21              0.043291            5.308814                 0.111063   \n",
       "22              0.042065            5.301239                 0.118073   \n",
       "23              0.034484            5.517565                 0.102825   \n",
       "24              0.044436            5.447720                 0.121886   \n",
       "25              0.039108            5.419865                 0.105182   \n",
       "26              0.047931            5.464250                 0.109447   \n",
       "27              0.064552            5.363120                 0.102492   \n",
       "28              0.028789            5.328347                 0.102987   \n",
       "29              0.024720            5.620069                 0.120170   \n",
       "30              0.031145            5.381082                 0.101511   \n",
       "31              0.021168            5.398140                 0.108424   \n",
       "32              0.015180            5.389597                 0.101628   \n",
       "33              0.028008            5.508913                 0.103480   \n",
       "34              0.026729            5.389274                 0.103658   \n",
       "35              0.016731            5.542999                 0.105630   \n",
       "36              0.010607            5.600379                 0.112655   \n",
       "37              0.026663            5.493910                 0.107017   \n",
       "38              0.006698            5.502132                 0.103188   \n",
       "39              0.018586            5.399477                 0.106888   \n",
       "40              0.026474            6.004318                 0.101703   \n",
       "41              0.008303            5.316276                 0.112128   \n",
       "42              0.008765            5.396958                 0.110426   \n",
       "43              0.009433            5.490754                 0.114794   \n",
       "44              0.006543            5.545204                 0.110141   \n",
       "45              0.009952            5.426411                 0.102252   \n",
       "46              0.009078            5.445199                 0.107120   \n",
       "47              0.008176            5.392847                 0.101435   \n",
       "48              0.006031            5.482871                 0.104584   \n",
       "49              0.012949            6.067215                 0.128765   \n",
       "\n",
       "    training_time_ALS  training_time_Bayesian  \n",
       "0            4.558881                1.282589  \n",
       "1            4.785893                1.175667  \n",
       "2            4.684738                1.153631  \n",
       "3            4.691514                1.176227  \n",
       "4            4.662004                1.154958  \n",
       "5            4.737060                1.134376  \n",
       "6            4.679289                1.174039  \n",
       "7            4.682302                1.204642  \n",
       "8            4.704821                1.199471  \n",
       "9            4.701531                1.142707  \n",
       "10           4.724844                1.200420  \n",
       "11           4.696148                1.140627  \n",
       "12           4.687505                1.203581  \n",
       "13           4.689091                1.185803  \n",
       "14           4.759310                1.164771  \n",
       "15           4.746042                1.224946  \n",
       "16           4.791874                1.223323  \n",
       "17           4.781446                1.223108  \n",
       "18           4.768883                1.195356  \n",
       "19           4.830125                1.188057  \n",
       "20           4.860843                1.158081  \n",
       "21           4.792480                1.214910  \n",
       "22           4.865003                1.205305  \n",
       "23           4.980561                1.211674  \n",
       "24           4.885733                1.147498  \n",
       "25           4.925035                1.191547  \n",
       "26           4.830178                1.161599  \n",
       "27           4.916381                1.203964  \n",
       "28           4.876835                1.290904  \n",
       "29           4.902338                1.144906  \n",
       "30           4.976239                1.159897  \n",
       "31           4.885539                1.149727  \n",
       "32           4.904422                1.270199  \n",
       "33           4.866037                1.220173  \n",
       "34           4.969409                1.184749  \n",
       "35           4.983228                1.448630  \n",
       "36           5.028212                1.162985  \n",
       "37           5.046439                1.175834  \n",
       "38           4.993618                1.301081  \n",
       "39           5.017410                1.271367  \n",
       "40           5.081787                1.188525  \n",
       "41           4.949132                1.228193  \n",
       "42           5.011008                1.218069  \n",
       "43           4.937163                1.191775  \n",
       "44           5.014268                1.141295  \n",
       "45           5.044679                1.198006  \n",
       "46           5.025216                1.186584  \n",
       "47           5.065249                1.222195  \n",
       "48           6.359135                1.797276  \n",
       "49           5.065703                1.208719  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [09:37<00:00, 11.56s/it]\n"
     ]
    }
   ],
   "source": [
    "log,sim,a_rec,b_rec=run_expt(50,n_users=1000,n_items=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
